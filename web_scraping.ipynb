{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='Text-align: center;'>**Web Scraping**</h1>\n",
    "\n",
    "`Created by: Erick Eduardo Robledo Montes`\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "<p style='Text-align: justify;'><i>Description:</i>  Web scraping data from the Mexican government's open data portal, located at https://datos.gob.mx/, using the Python libraries BeautifulSoup and Selenium. The script utilizes the capabilities of both libraries to navigate and extract the desired information from the website such as dataset titles and file formats. </p>\n",
    "\n",
    "*Link: [https://datos.gob.mx/]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes\n",
    "---\n",
    "* The `DataScraper` class is responsible for scraping data from the Mexican government's open data portal. This class likely contains the following functions:\n",
    "\n",
    "    - The `__init__` method, which is used to initialize the class and set any necessary variables, such as the URL of the website to scrape, and the web driver instance from Selenium.\n",
    "\n",
    "    - The `scrape_data` method, which is used to extract the desired information from the website, such as dataset titles, descriptions, and links, as well as statistics on the number of datasets and views. This method likely uses the BeautifulSoup library to parse the HTML structure of the website, and the Selenium web driver to interact with the website, navigate through the pages, and extract the data.\n",
    "\n",
    "    - The `display_stats` method, which is used to display the statistics of the scraping process.\n",
    "\n",
    "    - The `close_driver` method, which is used to close the Selenium web driver instance, and end the scraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataScraper:\n",
    "    \"\"\"\n",
    "    This class uses Selenium and BeautifulSoup to scrape data from the website \"https://datos.gob.mx/\".\n",
    "    It navigates to the website, searches for datasets with a specific keyword, and extracts the title and file format of each dataset.\n",
    "    The extracted data is then stored in a pandas dataframe and saved to a csv file.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the class variables and opens a webdriver for Selenium.\n",
    "        \"\"\"\n",
    "        self.data_name = \"datos_gob.csv\"\n",
    "        self.path = '\"../chromedriver/chromedriver.exe\"'\n",
    "        self.driver = webdriver.Chrome(self.path)\n",
    "        self.home_link = 'https://datos.gob.mx/'\n",
    "        self.search_kw = ''\n",
    "        self.search_url = '/busca/dataset?q='+self.search_kw+'&'\n",
    "        self.search_title = []\n",
    "        self.search_format = []\n",
    "        self.df = pd.DataFrame(columns=[\"dataset_title\", \"file_type\"])\n",
    "        \n",
    "    def scrape_data(self):\n",
    "        \"\"\"\n",
    "        Navigates to the website, iterates through the pages of search results, and extracts the title and file format of each dataset.\n",
    "        \"\"\"\n",
    "        self.driver.get(self.home_link + self.search_url)\n",
    "        page = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "\n",
    "        pagination = page.find('div', attrs={'class':'pagination'})\n",
    "        a_element = str(pagination.find_all('a')[-2])\n",
    "        pg_amount = int(re.sub(r'<[^>]*>', '', a_element))\n",
    "\n",
    "        for i in range(pg_amount):\n",
    "            self.driver.get(self.home_link + self.search_url)\n",
    "            page = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            for search in page.findAll('div', attrs={'class':'dataset-item'}):\n",
    "                if search:\n",
    "                    title = search.find('h3')\n",
    "                    file_formats = search.find_all('span', attrs={'dataset-format'})\n",
    "                    if file_formats:\n",
    "                        for file_format in file_formats:\n",
    "                            self.search_title.append(title.text)\n",
    "                            self.search_format.append(file_format.text)\n",
    "                    else: \n",
    "                        self.search_title.append(title.text)\n",
    "                        self.search_format.append('No Format.')\n",
    "                else: \n",
    "                    self.search_title.append(\"No Title.\")\n",
    "\n",
    "            self.search_url = '/busca/dataset?q='+self.search_kw+'&page='+str(i+2)\n",
    "\n",
    "            next_btn = self.driver.find_element(\n",
    "                By.CSS_SELECTOR, \n",
    "                'div.pagination.pagination-centered li:last-child a:not([class])')\n",
    "            next_btn.click()\n",
    "\n",
    "        self.search_title = [x.replace(\"\\n\", \"\") for x in self.search_title]\n",
    "        self.df = pd.DataFrame({\n",
    "            \"dataset_title\": self.search_title, \n",
    "            \"file_type\": self.search_format\n",
    "            })\n",
    "        self.df = self.df.drop_duplicates()\n",
    "        self.df.to_csv(self.data_name, index=None, header=True, encoding='utf-8-sig')\n",
    "\n",
    "    def display_stats(self):\n",
    "        \"\"\"\n",
    "        Prints the number of unique titles and file formats found in the search results.\n",
    "        \"\"\"\n",
    "        num_unique_titles = self.df[\"dataset_title\"].nunique()\n",
    "        print(num_unique_titles)\n",
    "        num_unique_files = self.df[\"file_type\"].nunique()\n",
    "        print(num_unique_files)\n",
    "\n",
    "    def close_driver(self):\n",
    "        \"\"\"\n",
    "        Close the webdriver.\n",
    "        \"\"\"\n",
    "        self.driver.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Data from a Website using Python's DataScraper Class\n",
    "---\n",
    "* The script of the following code creates an instance of the `DataScraper` class by calling the constructor.\n",
    "\n",
    "* The script then calls the `scrape_data` method on the `data_scraper` object to start the scraping process. This method likely uses the BeautifulSoup library to parse the HTML structure of the website, and the Selenium web driver to interact with the website, navigate through the pages, and extract the data.\n",
    "\n",
    "* After the scraping is done, the script calls the `display_stats` method on the `data_scraper` object to display the statistics of the scraping process, such as the number of datasets scraped and the number of views.\n",
    "\n",
    "* Finally, the script calls the `close_driver` method on the `data_scraper` object to close the Selenium web driver instance, and end the scraping process.\n",
    "\n",
    "* This script's purpose is to scrape data from a website and display the statistics of the scraping process and close the web driver instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scraper = DataScraper()\n",
    "data_scraper.scrape_data()\n",
    "data_scraper.display_stats()\n",
    "data_scraper.close_driver()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UASLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d296a6862911035b23eed23915f5ea7a3ddd49d93c9440154a78263fbdd13c4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
